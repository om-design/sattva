# SATTVA theory overview

SATTVA (Semantic Attractor Training of Transforming Vector Associations) is the documentation spine for an **Attractor-based Understanding Machine (AUM)**.

The working hypothesis: if semantic vectors are trained so that their associations form stable attractors in a dynamical system, then meaning, memory, and action selection can emerge from the geometry of those attractors.

## Dynamical systems and attractors

SATTVA treats an AI system as a **dynamical system**: a state that evolves over time according to fixed rules. Certain states or sets of states behave as **attractors**â€”the system tends to move toward them from many different starting points.

In this view, a "concept" or "understanding" is not a single vector, but a **stable pattern of activity**: a point attractor, a limit cycle, or a higher-dimensional attractor in the system's state space.

## Semantic vectors and associations

Modern AI systems often represent words, images, and concepts as **vectors** in a high-dimensional space. Nearby vectors tend to have related meanings; this is the basis of embeddings.

SATTVA assumes that these semantic vectors are not endpoints, but **ingredients** for a dynamical system. Learning adjusts how vectors influence each other so that meaningful patterns of association become stable attractors.

This overview is intended as the first stop for understanding what SATTVA is trying to build and why attractors and semantic vectors sit at its core.
